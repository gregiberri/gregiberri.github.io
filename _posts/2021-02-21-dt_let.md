---
layout: post
title: DT-LET - Deep Transfer Learning by Exploring where to Transfer
article: https://arxiv.org/abs/1809.08541
submission: 2018 Sep
description: In order to select the best matching of layers to transfer knowledge, they refine a specific loss to estimate the relationship between the features in the source and target domain.
tags: [transfer_learning, layer_matching, parameter_transfer]
---

For knowledge transfer between source and target domain parameter transfer approaches try to find pivot parameters to transfer to accelerate the transfer process.  


<br/><br/>
<strong>Problem Formulation:</strong> 
* source data $$D_{S}=\left\{I_{i}^{s}\right\}_{i=1}^{n_{s}}$$ (and some with labels $$D_{S}^{l}=\left\{X_{i}^{s}, Y_{i}^{s}\right\}_{i=1}^{n_{l}}$$)
* target data $$D_{T}=\left\{I_{i}^{t}\right\}_{i=1}^{n_{t}}$$
* co-occurences: data in the source domain and the target domain belonging to the same classes but with no prior label information $$D^{C}=\left\{C_{i}^{S}, C_{i}^{T}\right\}_{i}^{n_{c}}$$
* matching layers $$R_{s, t}=\left\{r_{i_{1}, j_{1}}^{1}, r_{i_{2}, j_{2}}^{2}, \ldots, r_{i_{a}, j_{b}}^{m}\right\}$$

![flowchart]({{ site.baseurl }}/images/dt_let/flowchart.png)

<br/><br/>
<strong>Deep mapping has steps:</strong> 
1. network setting up
2. correlation maximization
3. layer matching

<br/><br/>
<strong>Correlation maximization</strong>
* to set up the initial relationship of the two neural networks they resort to <i>Canonical Correlation Analysis</i> (CCA) which can maximize the correlation between two domains
* $$C_S$$ and $$C_T$$ are projected by CCA to a common subspace $$\Omega$$ (with projection matrices $$V^{S}(n) \text { and } V^{T}(n)$$) on which a uniformed representation is generated 
* to find the optimal neural network in the source and the target domain two general objectives:  

    1. minimize the reconstruction error of neural networks of the source domain and the target domain
    2. maximize the correlation between the two neural networks
<p align=center>$$L\left(R_{s, t}\right)=\frac{L_{s}\left(\theta^{S}\right)+L_{T}\left(\theta^{T}\right)}{P\left(V^{S}, V^{T}\right)}$$</p>

<br/><br/>
<strong>Layer matching</strong>
* different layer matching generate different loss $$L$$
* the matched layers yield the lowest loss (witch is found by trying out all combinations)
<p align=center>$$R_{s, t}=\arg \min L$$</p>

<br/><br/>
<strong>Model Training</strong>
1. updating $$V^{S}, V^{T}$$ with fixed $$\Theta^{S}, \Theta^{T}$$
2. updating $$\Theta^{S}, \Theta^{T}$$ with fixed $$V^{S}, V^{T}$$
![algorithm]({{ site.baseurl }}/images/dt_let/algorithm.png)

<br/><br/>
<strong>Results</strong>
![multi_feature_results]({{ site.baseurl }}/images/dt_let/multi_feature_results.png)
![multi_feature_matching]({{ site.baseurl }}/images/dt_let/multi_feature_matching.png)
<br/>
![nus_wide_results]({{ site.baseurl }}/images/dt_let/nus_wide_results.png)
![nus_wide_matching]({{ site.baseurl }}/images/dt_let/nus_wide_matching.png)


