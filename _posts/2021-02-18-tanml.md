---
layout: post
title: TANML - Task-similarity Aware Meta-learning through Nonparametric Kernel Regression
article: https://arxiv.org/abs/2006.07212
submission: 2020 Jun
description: Meta-learning framework that explicitly uses task similarity in the form of kernels and an associated meta-learing algorithm.
tags: [meta_learing, task_similarity, maml, meta_sgd]
---

Popular meta-learning approaches only rely on the availibility of huge number of tasks do not actively use the task similarity-dissimilarity.
In many cases the number of tasks is limited and task may not always be very similar or contain outlier or out of distribution tasks.

<br/><br/>
<strong>Preliminaries</strong>  
* tasks are given with train and test data pairs $$\mathcal{X}_{i}, \mathcal{Y}_{i}, \overline{\mathcal{X}}_{i}, \overline{\mathcal{Y}}_{i}, i=1, \cdots, T_{t r}$$ 
where $$\mathcal{X}=\left(x_{1}, x_{2}, \cdots, x_{K}\right) \text { and } \mathcal{Y}=\left(y_{1}, y_{2}, \cdots, y_{K}\right)$$
* we want to make a learing predictor $$\mathbb{R}^{n_{x}} \times \mathbb{R}^{D} \ni(x, \boldsymbol{\theta}) \mapsto f(x, \boldsymbol{\theta}) \in \mathbb{R}^{n_{y}}$$
* and minimize a loss function with respect to $$\boldsymbol{\theta}$$ $$\mathbb{R}^{K n_{x}} \times \mathbb{R}^{K n_{y}} \times \mathbb{R}^{D} \ni(\mathcal{X}, \mathcal{Y}, \boldsymbol{\theta}) \mapsto \mathcal{L}(\mathcal{X}, \mathcal{Y}, \boldsymbol{\theta}) \in \mathbb{R}$$
* meta learning captures similarities across tasks by learning common $$\boldsymbol{\theta}_{0}$$ that can be quickly adapted to train a predictor for data from a new task

<br/><br/>
<strong>From MAML to generalized meta-SGD</strong>  
In MAML we want to train our $$\boldsymbol{\theta}_{0}$$ initial parameterset.
![maml]({{ site.baseurl }}/images/tanml/maml.png)

In Meta-SGD we want to train both our $$\boldsymbol{\theta}_{0}$$ initial parameterset and $$\boldsymbol{\alpha}$$ parameterwise learning rate.
![msgd]({{ site.baseurl }}/images/tanml/msgd.png)

The inner update of meta-SGD can be expressed as $$g_{\mathrm{MSGD}}\left(\boldsymbol{\theta}_{0}, \boldsymbol{\alpha}, \mathcal{X}_{i}, \mathcal{Y}_{i}\right)=\mathbf{W}^{\top} \mathbf{z}_{i}\left(\boldsymbol{\theta}_{0}\right)$$
where
<p align=center>$$\mathbf{W} \triangleq[\mathbf{I},-\operatorname{diag}(\boldsymbol{\alpha})] \quad \text { and } \quad \mathbf{z}_{i}\left(\boldsymbol{\theta}_{0}\right) \triangleq\left[\boldsymbol{\theta}_{0}^{\top} \nabla_{\boldsymbol{\theta}_{0}}, \mathcal{L}\left(\mathcal{X}_{i}, \mathcal{Y}_{i}, \boldsymbol{\theta}_{0}\right)^{\top}\right]^{\top}$$</p>

Using this the generalized meta-SGD algorithm looks the following:
![gmsgd]({{ site.baseurl }}/images/tanml/gmsgd.png)

<br/><br/>
<strong>Task-similarity Aware Meta-Learning</strong>
![schematic]({{ site.baseurl }}/images/tanml/schematic.png)
Linear regression has a limited expressive power and the dimension of the linear regression matrix {\boldsymbol{W}} grows quadratically with the parameter number $$\boldsymbol{\theta}$$
Instead they used kernel regression with the kernel $$\mathbf{k}\left(\boldsymbol{\theta}_{0}, i\right) \triangleq
\left[k\left(\mathbf{z}_{i}\left(\boldsymbol{\theta}_{0}\right), \mathbf{z}_{1}\left(\boldsymbol{\theta}_{0}\right)\right), \cdots, k\left(\mathbf{z}_{i}\left(\boldsymbol{\theta}_{0}\right), \mathbf{z}_{T_{t r}}\left(\boldsymbol{\theta}_{0}\right)\right)\right]^{\top}$$
and $$\boldsymbol{\Psi}=\left[\boldsymbol{\psi}_{1}, \cdots, \boldsymbol{\psi}_{T_{t r}}\right]$$ matrix containing the kernel regression coefficients.
<p align=center>$$g_{\operatorname{TANML}}\left(\boldsymbol{\theta}_{0}, \boldsymbol{\Psi}, \mathcal{X}_{i}, \mathcal{Y}_{i}\right)=\sum_{i=1}^{T_{t r}} \psi_{i} k\left(\mathbf{z}_{i}\left(\boldsymbol{\theta}_{0}\right), \mathbf{z}_{i}\left(\boldsymbol{\theta}_{0}\right)\right)=\boldsymbol{\Psi}^{\top} \mathbf{k}\left(\boldsymbol{\theta}_{0}, i\right)$$</p>
![tanmsgd]({{ site.baseurl }}/images/tanml/tanmsgd.png)
They also added a $$\mu \Omega(\Psi)$$ regularization to promote smoothness and control overfitting 
where $$\Omega(\boldsymbol{\Psi})=\boldsymbol{\Psi}^{\top} \mathbf{K}\left(\boldsymbol{\theta}_{0}\right) \boldsymbol{\Psi}$$