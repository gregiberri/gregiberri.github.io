---
layout: post
title: GradDrop - Optimizing Deep Multitask Models with Gradient Sign Dropout
article: https://arxiv.org/abs/2010.06808
submission: 2020 Oct
description: Gradient dropout for problems with multiple losses, where they demand, that all gradient updates are pure in sign, with dropping out gradients of the other sign.
tags: [gradient_changing]
---

Gradiant signals are sums of many smaller gradient signals, usually corresponding to multiple losses. 
In multitask applications, the minima of each constituent loss may exist at different network weight settings, hence we need to find a joint minima - critical points that lie near a local minimum of all lossses.

When multiple gradients try to update the same scalar, conflicts arise through differences in sign, which leads to gradient-wars and critical points, where constituent gradients can still be large.
In order to solve this problem, they proposed Gradient Dropout, where gradient updates are pure in sign, which is done by algorithmically select a sign based on the distribution of gradient values, and mask out the gradients with the opposite sign.

![graddrop_schemantic]({{ site.baseurl }}/images/graddrop/graddrop_schemantic.png)

<br/><br/>
<strong>Problem Formulation:</strong>  
They use a gradient positive purity, to determine the positivity of gradients at a place:
<p align=center>$$\mathcal{P}=\frac{1}{2}\left(1+\frac{\sum_{i} \nabla L_{i}}{\sum_{i}\left|\nabla L_{i}\right|}\right)$$</p>

Then they make a mask for each gradient: 
<p align=center>$$\mathcal{M}_{i}=\mathcal{I}[f(\mathcal{P})>U] \circ \mathcal{I}\left[\nabla L_{i}>0\right]+\mathcal{I}[f(\mathcal{P})<U] \circ \mathcal{I}\left[\nabla L_{i}<0\right]$$</p>

<br/><br/>
<strong>The full algorithm:</strong>
![graddrop_algorithm]({{ site.baseurl }}/images/graddrop/graddrop_algorithm.png)

<br/><br/>
<strong>The results:</strong>  
Results on Multitask Learning: 
![result_multitask]({{ site.baseurl }}/images/graddrop/results_multitask.png)

Results on Transfer Learning:
![results_transfer]({{ site.baseurl }}/images/graddrop/results_transfer.png)
