---
layout: post
title: DEQ - Deep Equilibrium Models
article: https://arxiv.org/abs/1810.06682.pdf
submission: 2019 Sep
description: New approach to model sequential data. As the hidden layers of many existing deep sequence models converge towards some fixed point, DEQ approach finds and uses these equilibrium points via root-finding.
tags: ['deqs', 'weight_tied_networks', 'sequence_modeling']
---

Deep feedforward sequential model, where $$i$$ is the layer index:
<p align=center>$$z_{1:T}^{[i+1]}=f_{\Theta}^{[i]}(z_{1:T}^{[i]};x_{1:T})\quad for\; i=0, 1, ...L-1$$</p>

The paper derives from the observation that employing the same transformation in each layer converges to a fixed point: a point from which the activation does not change.

They find this fixed point $$z_{1:T}^{*}$$ of the non-linear equation with a root-finding method:
<p align=center>$$z_{1:T}^{*}=f_{\Theta}(z_{1:T}^{*};x_{1:T})$$</p>

<strong>Weight tying comes with four benefits: </strong>    
1. act as regularization that stabilizes training and supports generalization
2. reduction of model size
3. they proved that every deep network can be represented by a weight-tied network of equal depth and a linear increase of width
4. network can be unrolled to any depth, with improved feature abstraction as depth increases

<strong>Forward pass (finding the equilibrium point):</strong>  
The fixed-point iterations:
<p align=center>$$z_{1:T}^{[i+1]}=f_{\Theta}(z_{1:T}^{[i]};x_{1:T})\quad for\; i=1, 2, ...$$</p>
With the notation $$g_{\Theta}(z_{1:T}^{*};x_{1:T})=f_{\Theta}(z_{1:T}^{*};x_{1:T})-z_{1:T}^{*}\; \rightarrow 0$$, the equilibrium $$z_{1:T}^{*}$$ is the root of $$g_{\Theta}$$, which can be found with Newton's or quasi-Newton's methods (like Broyden):
<p align=center>$$z_{1:T}^{[i+1]}=z_{1:T}^{[i]}-\alpha B g_{\Theta}(z_{1:T}^{[i]};x_{1:T})$$</p>
Where $$B$$ is the Jacobian inverse and $$\alpha$$ is the step size.

<strong>Backward pass (training the network to have the equilibrium point at $$y_{1:T})$$ :</strong>  
The gradient of the equilibrium model to a $$y_{1:T}\in \mathbb{R}$$ groundtruth sequence (we want the equilibrium to be at $$y_{1:T})$$, where the model can be taught with any gradient based method:
<p align=center>$$l=\mathcal{L}(h(z_{1:T}^{*};y_{1:T})=\mathcal{L}(rootFind(g_{\Theta};x_{1:T})),y_{1:T})$$</p>

<strong>Memory cost:</strong>  
For both the forward and backward pass DEQ needs to store only $$z_{1:T}^*$$, $$x_{1:T}$$ and $$f_{\Theta}$$ for the backward pass.  
For conventional deep networks with $$L$$ layers, the training memory is $$\mathcal{O}(L)$$, while for DEQ it is $$\mathcal{O}(1)$$ due to the root finding operation.

The results are:
![results]({{ site.baseurl }}/images/deq/results.png)

<strong>Notes: </strong>  
* Stacking the DEQs does not create extra representation power over a single DEQ.  
* Computation depends on the Broyden iteration number, but a bad estimation of the equilibrium can entail bad accuracy or even divergence.  
* DEQs are typically slower than layer-based deep networks
