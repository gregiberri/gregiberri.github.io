---
layout: post
title: Trellis Networks for Sequence Modelling
article: https://arxiv.org/abs/1810.06682.pdf
submission: 2018 Oct
description: New architecture for sequential modeling. Temporal convolutional network with weight tying which is equiavent to truncated recurrent networks.
tags: ['recurrent_networks', 'sequence_modeling']
---

It is a special temporal convolutional network, distinguished by 2 characteristics: 
* weights are shared not only by all time steps but all network layers too
* input is injected into all network layers

The feature $$z_{t+1}^{(i+1)}$$ is computed with the following steps:
![interlayer_transformation]({{ site.baseurl }}/images/trellis/interlayer_transformation.png)
1. Hidden input comprises the hidden outputs $$z_{t}^{(i)}$$ $$z_{t+1}^{(i+1)}$$ âˆˆ $$\mathbb{R}^{q}$$ from the previous layer $$i$$, as well the injection of the input vectors $$x_t$$, $$x_{t+1}$$. (At level 0 $$z_{t}^{(0)}$$ is initialized to 0.)
2. Pre-activation output $$\tilde{z}_{t+1}^{(i+1)} \in \mathbb{R}^{r}$$ is produced by a feed forward linear transformation:  
![function_0]({{ site.baseurl }}/images/trellis/function_0.png)

3. The and output $$z_{t+1}^{(i+1)}$$ is produced by a nonlinear function applied to the pre-actication output $$\tilde{z}_{t+1}^{(i+1)}$$ and output $$z_{t}^{(i)}$$:  
<p align=center>$$z_{t+1}^{(i+1)}=f(\tilde{z}_{t+1}^{(i+1)}, z_{t}^{(i)})$$</p>

In each level of the network they are in effect performing a 1D convolution over the hidden units $$z_{1:T}^{(i)}$$:
<p align=center>$$\tilde{z}_{1:T}^{(i+1)}=Conv1D(z_{1:T}^{(i)},W)+W_{1}^{x}x_{t}+W_{2}^{x}x_{t+1}$$</p>  
<p align=center>$$z_{1:T}^{(i+1)}=f(\tilde{z}_{1:T}^{(i+1)}, z_{1:T-1}^{(i)})$$</p>

Here the deeper elements have progressively larger receptive fields.

Any truncated RNN can be represented as TrellisNet with special structure in the interlayer transformations.

The used $$f$$ function can be any nonlinearity: they used gated activations based on the LSTM cells.

In LSTM there are 3 information controlling gates plus one gate which does not participate in the hidden-to-hidden transformations but is updated in every step using the result from the gated activations. The LSCT cell was integrated into the TrellisNEt the following way:
![function_1]({{ site.baseurl }}/images/trellis/function_1.png)
