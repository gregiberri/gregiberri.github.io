---
layout: post
title: BYOL - Bootstrap your own latent
article: https://arxiv.org/abs/2006.07733
submission: 2020 jun
description: New approach for self-supervised image representation learning. Use two netoworks, an online and a target network that are trained to predict the representation of the same image with different augmentation. 
tags: [representation_learning, unsupervised_learning, self_supervised_learning]
---

They introduced a new self-supervised algorithm which learns representations without negative pairs: it iterativfely bootstraps the outputs of a network to serve as targets for an enahnced representation.

<br/><br/>
<strong>Method</strong>  
Contrastive learning algorithms needs many negative examples to find ones sufficiently close to make the discrimination task challenging  
BYOL uses two neural networks: the <i>online</i> and the <i>target</i> network.
![architecture]({{ site.baseurl }}/images/byol/architecture.png)

* three stages: <i>encoder</i> $$f_{\theta}$$, <i>projector</i> $$g_{\theta}$$ and <i>predictor</i> $$g_{\theta}$$
* <i>online</i> and <i>target</i> network has the same architecture with weight $$\theta$$ and $$\xi$$
* <i>target</i> network provides the regression targets to train the <i>online</i> network 
* the <i>target<\i> network's parameters are trained only with an exponential moving average of the <i>online</i> parameters $$\theta$$
<p align=center>$$\xi \leftarrow \tau \xi+(1-\tau) \theta$$</p>
* the <i>target</i> and <i>online</i> network receives the same input with different augmentation
* the loss is an MSE between the normalized predictions and target projections
<p align=center>$$\mathcal{L}_{\theta, \xi} \triangleq\left\|\overline{q_{\theta}}\left(z_{\theta}\right)-\bar{z}_{\xi}^{\prime}\right\|_{2}^{2}=2-2 \cdot \frac{\left\langle q_{\theta}\left(z_{\theta}\right), z_{\xi}^{\prime}\right\rangle}{\left\|q_{\theta}\left(z_{\theta}\right)\right\|_{2} \cdot\left\|z_{\xi}^{\prime}\right\|_{2}}$$</p>
* the loss is symmetrized by separately feeding $$v'$$ to the <i>online</i> and $$v$$ to the <i>target</i> network to compute $$\tilde{\mathcal{L}}_{\theta, \xi}$$
* the whole training loss is $$\mathcal{L}_{\theta, \xi}^{\mathrm{BYOL}}=\mathcal{L}_{\theta, \xi}+\widetilde{\mathcal{L}}_{\theta, \xi}$$

<br/><br/>
<strong>The training algorithm</strong>  
<p align=center>$$\begin{array}{l}
\theta \leftarrow \text { optimizer }\left(\theta, \nabla_{\theta} \mathcal{L}_{\theta, \xi}^{\mathrm{BYOL}}, \eta\right) \\
\xi \leftarrow \tau \xi+(1-\tau) \theta
\end{array}$$</p>

<br/><br/>
<strong>Results</strong>
![results]({{ site.baseurl }}/images/byol/results.png)
