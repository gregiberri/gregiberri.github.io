---
layout: post
title: SimCLR - Simple Framework for Contrastive Learning of Visual Representations
article: https://arxiv.org/abs/2002.05709
submission: 2020 Feb
description: SimCLR is a framework, which uses composition of data augmentations for contrastive learning, with utilizing additional findings.
tags: [contrastive_learning, representation_learning, unsupervised_learning, self_supervised_learning]
---

They introduced a contrastive learning framework, which use a composition of data augmentations to learn effective representations.
 ![framework]({{ site.baseurl }}/images/simclr/framework.png)

<br/><br/>
SimCLR learns representations by maximizing agreement between differently augmented views of the same dats example via contrastive loss in latent space:
1. <i>stochastic data augmentation</i> transforms given data examples randomly resulting 2 views of the same example (the positive pair) $$\tilde{\boldsymbol{x}}_{i}$$ and $$\tilde{\boldsymbol{x}}_{j}$$
2. all sample pairs from different pairs are considered <i>negative pairs</i>
3. neural network <i>base encoder</i> $$f(\cdot)$$ extract representation vectors
4. small network <i>projection head</i> maps representations to the space of the contrastive loss
5. <i>contrastive loss</i> is calculated: <i>contrastive prediction task</i> aims to identify $$\tilde{\boldsymbol{x}}_{j}$$ in $$\{\tilde{\boldsymbol{x}}_{k}\}_{k\neq i}$$ that includes $$\tilde{\boldsymbol{x}}_{i}$$ and $$\tilde{\boldsymbol{x}}_{j}$$

After training the representations the <i>projection head</i> is removed and only the encoder is used.

The similarity mesurement is calculated from the $$l_2$$ normalized $$u$$ and $$v$$: 
$$\operatorname{sim}(\boldsymbol{u}, \boldsymbol{v})=\boldsymbol{u}^{\top} \boldsymbol{v} /\|\boldsymbol{u}\|\|\boldsymbol{v}\|$$ with the NT-Xent loss (normalized temperature-scaled cross entropy loss):
<p align=center>$$\ell_{i, j}=-\log \frac{\exp \left(\operatorname{sim}\left(z_{i}, z_{j}\right) / \tau\right)}{\sum_{k=1}^{2 N} \mathbb{1}_{[k \neq i]} \exp \left(\operatorname{sim}\left(z_{i}, z_{k}\right) / \tau\right)}$$</p>

<br/><br/>
<strong>The algorithm:</strong> 
 ![algorithm]({{ site.baseurl }}/images/simclr/algorithm.png)

<br/><br/>
<strong>Data augmentation composition</strong>
* best composition of augmentation: first random crop, then random color distortion
* most patches from an image share similar color distribution: neural networks would exploit this shortcut
* stronger color augmentation improves the performance

<br/><br/>
<strong>Unsupervised contrastive learning benefits more from bigger models</strong>
* as the model size increases the gap closes between the supervised and unsupervised: unsupervised learning benefits more from bigger netoworks 
![bigger_model]({{ site.baseurl }}/images/simclr/bigger_model.png)

<br/><br/>
<strong>Projection head</strong>
* the best performance can be achieved with nonlinear projection head, and the worst with no projection head
![projection_head]({{ site.baseurl }}/images/simclr/projection_head.png)

<br/><br/>
<strong>Loss function</strong>
* the best performance can be achieved with NT-Xent loss
![losses]({{ site.baseurl }}/images/simclr/losses.png)

<br/><br/>
<strong>Batch size</strong>
* when the number of training epochs is small, then larger batch sizes have significant advantage
* when the number of training epochs is large, the difference decrease
![batch_number]({{ site.baseurl }}/images/simclr/batch_number.png)

<br/><br/>
<strong>Results</strong>
![results]({{ site.baseurl }}/images/simclr/results.png)