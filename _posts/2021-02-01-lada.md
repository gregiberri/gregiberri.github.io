---
layout: post
title: LADA - Look-ahead data acquisition via augmentatnion for active learning
article: https://arxiv.org/abs/2011.04194
submission: 2020 Nov
description: Active learning with utilizing the predicted added value of augmentation.
tags: ['active_learning', data_augmentation]
---

<strong>Problem Formulation:</strong>  
* network: $$f_{\theta}$$
* dataset containing labeled and unlabeled data: $$\mathscr{X} = \mathscr{X}_{U} \cup \mathscr{X}_{L}$$ and $$\mathscr{X}_{U} \gg \mathscr{X}_{L}$$
* data augmentation: $$f_{aug}(x, \tau): \mathscr{X} \rightarrow V(\mathscr{X})$$
* data acquisition function: $$f_{acq}(x, f_{\theta}): \mathscr{X_U} \rightarrow \mathbb{R}$$

<strong>Goal of LADA is to enhance informativeness of:</strong>
1. <strong>currently unlabelled real-world data</strong> instance (but to be labelled by oracle)
2. <strong>virtual data instance</strong>, generated <strong>by augmenting</strong> the unlabeled data instance for which they used input mixup (<i>InfoMixup</i>) and layer mixup (<i>Manifold Mixup</i>) augmentations
![different_scenarios]({{ site.baseurl }}/images/lada/different_scenarios.png)

<strong>LADA trains the data augmentation function</strong> $$f_{aug}(x, \tau)$$ to maximize the acquisition score of the transformed data $$x_{U}$$. The optimal $$\tau^{*}$$ augmentation policy:

<p align=center>$$\tau^{*}=\underset{\tau}{\operatorname{argmax}} f_{a c q}\left(f_{a u g}\left(x_{U} ; \tau\right) ; f_{\theta}\right)$$</p>

<strong>The optimal augmentation to maximize the acquisition score is learned</strong>: the  mixup policy for mixing inputs (<i>InfoMixup</i>) and also used a policy generator network, that predicts $$\tau_{i}^{*}$$: 
the optimal policy for mixing up layers from 2 different inputs (<i>Manifold Mixup</i>) to maximize the expected acquisition score ($$\mathbb{H}$$: <i>Entropy</i>)
![policy_generator]({{ site.baseurl }}/images/lada/policy_generator.png)

The optimal $$x_{U}$$ to be labelled: 
<p align=center>$$x_{U}^{*}=\underset{x_{U} \in \mathscr{X}_{U}}{\operatorname{argmax}}\left[f_{a c q}\left(x_{U} ; f_{\theta}\right)+f_{a c q}\left(f_{a u g}\left(x_{U} ; \tau^{*}\right) ; f_{\theta}\right)\right]$$</p>

<strong>After acquiring the augmentation policy they get the joint acquisition score</strong> from the first $$x_{i}$$, and second $$x_{i}'$$ input and their mixed feature maps $$h_{m}^{k, n}(x_{i}, x{i}')$$ by calculating the respective predictive entropy:
<p align=center>$$f_{a c q}((x_{i}, x_{i}^{\prime}) ; f_{\theta})=\mathbb{H}[\hat{y}_{i} | x_{i} ; f_{\theta}]+\mathbb{H}[\hat{y}_{i}^{\prime} | x_{i}^{\prime} ; f_{\theta}]+\frac{1}{N} \sum_{n}^{N} \mathbb{H}[\hat{y}_{i}^{n} |  h_{m}^{k, n}(x_{i}, x_{i}^{\prime}) ; f_{\theta}^{k: L}]$$</p>

Starting at the $$j^{th}$$ iteration from having $$\mathscr{X}_{L}^{j}$$ labeled dataset, <strong>the method tries to acquire the top k/2 pairs: $$\mathscr{X}_{S}^{\prime} \subset \mathscr{X}_{U} \times \mathscr{X}_{U}, \text { with }\left|\mathscr{X}_{S}^{\prime}\right|=\frac{k}{2}$$</strong>
and we also get the $$\mathscr{X}_{M}$$ virtual data from the augmentation. 

<strong>After the oracle's labeling of the new data the dataset becomes:</strong> $$\mathscr{X}_{L}^{j+1}=\mathscr{X}_{L}^{j} \cup (\mathscr{X}_{S} \; and \; \mathscr{X}_{M})$$

The whole algorithm:
![algorithm]({{ site.baseurl }}/images/lada/algorithm.png)

And the results:
![results]({{ site.baseurl }}/images/lada/results.png)
