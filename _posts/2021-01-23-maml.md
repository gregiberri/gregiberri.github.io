---
layout: post
title: MAML - Model-Agnostic Meta-Learning
article: https://arxiv.org/abs/1703.03400
submission: 2017 Mar
description: Train a model’s initial parameters  such that the model has maximal performance on new task after only a few gradient updates by maximizing the sensitivity to  the loss functions of new tasks.
tags: ['meta_learning']
---

Goal: to train a model’s initial parameters  such that the model has maximal performance on new task after only a few gradient updates: maximizing the sensitivity to  the loss functions of new tasks.

The updated parameter vector is computed using one or more gradient descent updates on task Ti:

![function_0]({{ site.baseurl }}/images/maml/function_0.png)

Model parameters are trained by optimizing for the performance of f_theta_prime_i with respect to theta across tasks sampled:

![function_1]({{ site.baseurl }}/images/maml/function_1.png)

Which is done with gradient through gradient, which requires to calculate the Hessian vector.

![function_2]({{ site.baseurl }}/images/maml/function_2.png)

The algorithm:

![algorithm]({{ site.baseurl }}/images/maml/algorithm.png)

